{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import animeface\n",
    "import keras.backend as K\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras import Sequential, Input, Model\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from pathlib import Path\n",
    "from scipy.misc import imread, imsave\n",
    "from scipy.stats import entropy\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping and resizing images in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_faces = 0\n",
    "raw_image_dir = str(Path(os.getcwd())/\"data\"/\"gallery-dl\"/\"danbooru\"/\"face\")\n",
    "output_dir = Path(os.getcwd())/\"data\"\n",
    "for index, filename in enumerate(glob.glob(raw_image_dir+'/\"*')):\n",
    "    try:\n",
    "        im = Image.open(filename)\n",
    "        faces = animeface.detect(im)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception:{e}\")\n",
    "        continue\n",
    "    \n",
    "    # If no faces found in the image\n",
    "    if len(faces) == 0:\n",
    "        print(f\"No faces found in image {filename}\")\n",
    "    \n",
    "    fp = faces[0].face.pos\n",
    "\n",
    "    # Get coordinates of face detected in x1, y1, x2, y2 format\n",
    "    coordinates = (fp.x, fp.y, fp.x+fp.width, fp.y+fp.height)\n",
    "\n",
    "    # Crop image\n",
    "    cropped_image = im.crop(coordinates)\n",
    "\n",
    "    # Resize Image\n",
    "    cropped_image = cropped_image.resize((64,64), Image.ANTIALIAS)\n",
    "\n",
    "    # Save to output directory\n",
    "    cropped_image.save(output_dir/filename)\n",
    "\n",
    "    total_num_faces += 1\n",
    "    print(f\"Number of faces detected till now: {total_num_faces}\")\n",
    "\n",
    "print(f\"Total number of faces: {total_num_faces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    gen_model = Sequential()\n",
    "\n",
    "    gen_model.add(Dense(input_dim=100, output_dim=2048))\n",
    "    gen_model.add(ReLU())\n",
    "\n",
    "    gen_model.add(Dense(256 * 8 * 8))\n",
    "    gen_model.add(BatchNormalization())\n",
    "    gen_model.add(ReLU())\n",
    "    gen_model.add(Reshape((8, 8, 256), input_shape=(256 * 8 * 8,)))\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    gen_model.add(Conv2D(128, (5, 5), padding='same'))\n",
    "    gen_model.add(ReLU())\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    gen_model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    gen_model.add(ReLU())\n",
    "\n",
    "    gen_model.add(UpSampling2D(size=(2, 2)))\n",
    "\n",
    "    gen_model.add(Conv2D(3, (5, 5), padding='same'))\n",
    "    gen_model.add(Activation('tanh'))\n",
    "    return gen_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    dis_model = Sequential()\n",
    "    dis_model.add(\n",
    "        Conv2D(128, (5, 5),\n",
    "               padding='same',\n",
    "               input_shape=(64, 64, 3))\n",
    "    )\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    dis_model.add(Conv2D(256, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    dis_model.add(Conv2D(512, (3, 3)))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "    dis_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    dis_model.add(Flatten())\n",
    "    dis_model.add(Dense(1024))\n",
    "    dis_model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    dis_model.add(Dense(1))\n",
    "    dis_model.add(Activation('sigmoid'))\n",
    "\n",
    "    return dis_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_adversarial_model(gen_model, dis_model):\n",
    "    model = Sequential()\n",
    "    model.add(gen_model)\n",
    "    dis_model.trainable = False\n",
    "    model.add(dis_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, name, loss, batch_no):\n",
    "    \"\"\"\n",
    "    Write training summary to TensorBoard\n",
    "    \"\"\"\n",
    "    # for name, value in zip(names, logs):\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = loss\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inception_score(images_path, batch_size=1, splits=10):\n",
    "    # Create an instance of InceptionV3\n",
    "    model = InceptionResNetV2()\n",
    "\n",
    "    images = None\n",
    "    for image_ in glob.glob(images_path):\n",
    "        # Load image\n",
    "        loaded_image = image.load_img(image_, target_size=(299, 299))\n",
    "\n",
    "        # Convert PIL image to numpy ndarray\n",
    "        loaded_image = image.img_to_array(loaded_image)\n",
    "\n",
    "        # Another another dimension (Add batch dimension)\n",
    "        loaded_image = np.expand_dims(loaded_image, axis=0)\n",
    "\n",
    "        # Concatenate all images into one tensor\n",
    "        if images is None:\n",
    "            images = loaded_image\n",
    "        else:\n",
    "            images = np.concatenate([images, loaded_image], axis=0)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = (images.shape[0] + batch_size - 1) // batch_size\n",
    "\n",
    "    probs = None\n",
    "\n",
    "    # Use InceptionV3 to calculate probabilities\n",
    "    for i in range(num_batches):\n",
    "        image_batch = images[i * batch_size:(i + 1) * batch_size, :, :, :]\n",
    "        prob = model.predict(preprocess_input(image_batch))\n",
    "\n",
    "        if probs is None:\n",
    "            probs = prob\n",
    "        else:\n",
    "            probs = np.concatenate([prob, probs], axis=0)\n",
    "\n",
    "    # Calculate Inception scores\n",
    "    divs = []\n",
    "    split_size = probs.shape[0] // splits\n",
    "\n",
    "    for i in range(splits):\n",
    "        prob_batch = probs[(i * split_size):((i + 1) * split_size), :]\n",
    "        p_y = np.expand_dims(np.mean(prob_batch, 0), 0)\n",
    "        div = prob_batch * (np.log(prob_batch / p_y))\n",
    "        div = np.mean(np.sum(div, 1))\n",
    "        divs.append(np.exp(div))\n",
    "\n",
    "    return np.mean(divs), np.std(divs)\n",
    "\n",
    "\n",
    "def calculate_mode_score(gen_images_path, real_images_path, batch_size=32, splits=10):\n",
    "    # Create an instance of InceptionV3\n",
    "    model = InceptionResNetV2()\n",
    "\n",
    "    # Load real images\n",
    "    real_images = None\n",
    "    for image_ in glob.glob(real_images_path):\n",
    "        # Load image\n",
    "        loaded_image = image.load_img(image_, target_size=(299, 299))\n",
    "\n",
    "        # Convert PIL image to numpy ndarray\n",
    "        loaded_image = image.img_to_array(loaded_image)\n",
    "\n",
    "        # Another another dimension (Add batch dimension)\n",
    "        loaded_image = np.expand_dims(loaded_image, axis=0)\n",
    "\n",
    "        # Concatenate all images into one tensor\n",
    "        if real_images is None:\n",
    "            real_images = loaded_image\n",
    "        else:\n",
    "            real_images = np.concatenate([real_images, loaded_image], axis=0)\n",
    "\n",
    "    # Load generated images\n",
    "    gen_images = None\n",
    "    for image_ in glob.glob(gen_images_path):\n",
    "        # Load image\n",
    "        loaded_image = image.load_img(image_, target_size=(299, 299))\n",
    "\n",
    "        # Convert PIL image to numpy ndarray\n",
    "        loaded_image = image.img_to_array(loaded_image)\n",
    "\n",
    "        # Another another dimension (Add batch dimension)\n",
    "        loaded_image = np.expand_dims(loaded_image, axis=0)\n",
    "\n",
    "        # Concatenate all images into one tensor\n",
    "        if gen_images is None:\n",
    "            gen_images = loaded_image\n",
    "        else:\n",
    "            gen_images = np.concatenate([gen_images, loaded_image], axis=0)\n",
    "\n",
    "    # Calculate number of batches for generated images\n",
    "    gen_num_batches = (gen_images.shape[0] + batch_size - 1) // batch_size\n",
    "    gen_images_probs = None\n",
    "    # Use InceptionV3 to calculate probabilities of generated images\n",
    "    for i in range(gen_num_batches):\n",
    "        image_batch = gen_images[i * batch_size:(i + 1) * batch_size, :, :, :]\n",
    "        prob = model.predict(preprocess_input(image_batch))\n",
    "\n",
    "        if gen_images_probs is None:\n",
    "            gen_images_probs = prob\n",
    "        else:\n",
    "            gen_images_probs = np.concatenate([prob, gen_images_probs], axis=0)\n",
    "\n",
    "    # Calculate number of batches for real images\n",
    "    real_num_batches = (real_images.shape[0] + batch_size - 1) // batch_size\n",
    "    real_images_probs = None\n",
    "    # Use InceptionV3 to calculate probabilities of real images\n",
    "    for i in range(real_num_batches):\n",
    "        image_batch = real_images[i * batch_size:(i + 1) * batch_size, :, :, :]\n",
    "        prob = model.predict(preprocess_input(image_batch))\n",
    "\n",
    "        if real_images_probs is None:\n",
    "            real_images_probs = prob\n",
    "        else:\n",
    "            real_images_probs = np.concatenate([prob, real_images_probs], axis=0)\n",
    "\n",
    "    # KL-Divergence: compute kl-divergence and mean of it\n",
    "    num_gen_images = len(gen_images)\n",
    "    split_scores = []\n",
    "\n",
    "    for j in range(splits):\n",
    "        gen_part = gen_images_probs[j * (num_gen_images // splits): (j + 1) * (num_gen_images // splits), :]\n",
    "        real_part = real_images_probs[j * (num_gen_images // splits): (j + 1) * (num_gen_images // splits), :]\n",
    "        gen_py = np.mean(gen_part, axis=0)\n",
    "        real_py = np.mean(real_part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(gen_part.shape[0]):\n",
    "            scores.append(entropy(gen_part[i, :], gen_py))\n",
    "\n",
    "        split_scores.append(np.exp(np.mean(scores) - entropy(gen_py, real_py)))\n",
    "\n",
    "    final_mean = np.mean(split_scores)\n",
    "    final_std = np.std(split_scores)\n",
    "\n",
    "    return final_mean, final_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    img = (img + 1) * 127.5\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    return (img - 127.5) / 127.5\n",
    "\n",
    "\n",
    "def visualize_rgb(img):\n",
    "    \"\"\"\n",
    "    Visualize a rgb image\n",
    "    :param img: RGB image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_rgb_img(img, path):\n",
    "    \"\"\"\n",
    "    Save a rgb image\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"RGB Image\")\n",
    "\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start_time = time.time()\n",
    "    dataset_dir = \"data/*.*\"\n",
    "    batch_size = 128\n",
    "    z_shape = 100\n",
    "    epochs = 10000\n",
    "    dis_learning_rate = 0.005\n",
    "    gen_learning_rate = 0.005\n",
    "    dis_momentum = 0.5\n",
    "    gen_momentum = 0.5\n",
    "    dis_nesterov = True\n",
    "    gen_nesterov = True\n",
    "\n",
    "    dis_optimizer = SGD(lr=dis_learning_rate, momentum=dis_momentum, nesterov=dis_nesterov)\n",
    "    gen_optimizer = SGD(lr=gen_learning_rate, momentum=gen_momentum, nesterov=gen_nesterov)\n",
    "\n",
    "    # Load images\n",
    "    all_images = []\n",
    "    for index, filename in enumerate(glob.glob(dataset_dir)):\n",
    "        all_images.append(imread(filename, flatten=False, mode='RGB'))\n",
    "\n",
    "    X = np.array(all_images)\n",
    "    X = (X - 127.5) / 127.5\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    dis_model = build_discriminator()\n",
    "    dis_model.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "    gen_model = build_generator()\n",
    "    gen_model.compile(loss='mse', optimizer=gen_optimizer)\n",
    "\n",
    "    adversarial_model = build_adversarial_model(gen_model, dis_model)\n",
    "    adversarial_model.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time.time()), write_images=True, write_grads=True, write_graph=True)\n",
    "    tensorboard.set_model(gen_model)\n",
    "    tensorboard.set_model(dis_model)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"--------------------------\")\n",
    "        print(\"Epoch:{}\".format(epoch))\n",
    "\n",
    "        dis_losses = []\n",
    "        gen_losses = []\n",
    "\n",
    "        num_batches = int(X.shape[0] / batch_size)\n",
    "\n",
    "        print(\"Number of batches:{}\".format(num_batches))\n",
    "        for index in range(num_batches):\n",
    "            print(\"Batch:{}\".format(index))\n",
    "\n",
    "            z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "            # z_noise = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "\n",
    "            generated_images = gen_model.predict_on_batch(z_noise)\n",
    "\n",
    "            # visualize_rgb(generated_images[0])\n",
    "\n",
    "            \"\"\"\n",
    "            Train the discriminator model\n",
    "            \"\"\"\n",
    "\n",
    "            dis_model.trainable = True\n",
    "\n",
    "            image_batch = X[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            y_real = np.ones((batch_size, )) * 0.9\n",
    "            y_fake = np.zeros((batch_size, )) * 0.1\n",
    "\n",
    "            dis_loss_real = dis_model.train_on_batch(image_batch, y_real)\n",
    "            dis_loss_fake = dis_model.train_on_batch(generated_images, y_fake)\n",
    "\n",
    "            d_loss = (dis_loss_real+dis_loss_fake)/2\n",
    "            print(\"d_loss:\", d_loss)\n",
    "\n",
    "            dis_model.trainable = False\n",
    "\n",
    "            \"\"\"\n",
    "            Train the generator model(adversarial model)\n",
    "            \"\"\"\n",
    "            z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "            # z_noise = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "\n",
    "            g_loss = adversarial_model.train_on_batch(z_noise, y_real)\n",
    "            print(\"g_loss:\", g_loss)\n",
    "\n",
    "            dis_losses.append(d_loss)\n",
    "            gen_losses.append(g_loss)\n",
    "\n",
    "        \"\"\"\n",
    "        Sample some images and save them\n",
    "        \"\"\"\n",
    "        if epoch % 100 == 0:\n",
    "            z_noise = np.random.normal(0, 1, size=(batch_size, z_shape))\n",
    "            gen_images1 = gen_model.predict_on_batch(z_noise)\n",
    "\n",
    "            for img in gen_images1[:2]:\n",
    "                save_rgb_img(img, \"results/one_{}.png\".format(epoch))\n",
    "\n",
    "        print(\"Epoch:{}, dis_loss:{}\".format(epoch, np.mean(dis_losses)))\n",
    "        print(\"Epoch:{}, gen_loss: {}\".format(epoch, np.mean(gen_losses)))\n",
    "\n",
    "        \"\"\"\n",
    "        Save losses to Tensorboard after each epoch\n",
    "        \"\"\"\n",
    "        write_log(tensorboard, 'discriminator_loss', np.mean(dis_losses), epoch)\n",
    "        write_log(tensorboard, 'generator_loss', np.mean(gen_losses), epoch)\n",
    "\n",
    "    \"\"\"\n",
    "    Save models\n",
    "    \"\"\"\n",
    "    gen_model.save(\"generator_model.h5\")\n",
    "    dis_model.save(\"generator_model.h5\")\n",
    "\n",
    "    print(\"Time:\", (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
